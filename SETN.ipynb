{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # 娛樂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=8&p=%s\"\n",
    "\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_page(url):\n",
    "    resp = requests.get(url)\n",
    "    return resp.text\n",
    "#建立get_page方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save2file(filename, text):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(text)\n",
    "#建立save方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/娛樂/list_page\n",
    "#新建資料夾為儲存所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=8&p=%s\"\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(\"[INFO]crawling %s\"%url)\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/娛樂/list_page/' + \"%s.html\"%page, text)\n",
    "    \n",
    "#透過上序方法將跑出的所有分頁儲存在資料夾中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "\n",
    "glob.glob('/home/ubuntu/SETN_data/娛樂/list_page/*')\n",
    "\n",
    "res = []\n",
    "for filename in glob.glob('/home/ubuntu/SETN_data/娛樂/list_page/*'):\n",
    "    with open(filename) as f:\n",
    "        html = f.read()\n",
    "        detail_urls = re.findall('''(/E/News.aspx\\?NewsID=.+)\" style.+''', html)\n",
    "        res.extend(detail_urls)\n",
    "        #/E/News.aspx?NewsID=377934\n",
    "\n",
    "res\n",
    "\n",
    "#抓取位於/home/ubuntu/SETN_data/list_page/裡所有資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/娛樂/detail_page\n",
    "#新增一資料夾為儲存內工作內容所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in res:\n",
    "    filename =link.split('=')[1]\n",
    "    url = 'http://www.setn.com/E/News.aspx?NewsID='+ filename\n",
    "    print(url)\n",
    "    \n",
    "    # using the fucntions defined above ^^^^^^^^^^^^^\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/娛樂/detail_page/' + filename, text)\n",
    "    #透過分詞重新整理網址內容為最簡短化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml') # html5lib\n",
    "soup\n",
    "#使用beautifulsoup察看資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {}\n",
    "for paper_data in glob.glob('/home/ubuntu/SETN_data/娛樂/detail_page/*'):\n",
    "    with open(paper_data) as t:\n",
    "        html = t.read()\n",
    "    paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]\n",
    "    detail_paper = \"  \"\n",
    "    for i in paper :\n",
    "        each_paper = str(i).split('>')[1].split('<')[0]\n",
    "        detail_paper = detail_paper + each_paper\n",
    "        name = paper_data.split('page/')[1]\n",
    "        res_dict[name] = detail_paper\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 政治"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=6&p=%s\"\n",
    "\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/政治/list_page\n",
    "#新建資料夾為儲存所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=6&p=%s\"\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(\"[INFO]crawling %s\"%url)\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/政治/list_page/' + \"%s.html\"%page, text)\n",
    "    \n",
    "#透過上序方法將跑出的所有分頁儲存在資料夾中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "\n",
    "glob.glob('/home/ubuntu/SETN_data/政治/list_page/*')\n",
    "\n",
    "res = []\n",
    "for filename in glob.glob('/home/ubuntu/SETN_data/政治/list_page/*'):\n",
    "    with open(filename) as f:\n",
    "        html = f.read()\n",
    "        detail_urls = re.findall('''(/News.aspx\\?NewsID=.+)\" style.+''', html)\n",
    "        res.extend(detail_urls)\n",
    "        #/News.aspx?NewsID=377721\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/政治/detail_page\n",
    "#新增一資料夾為儲存內工作內容所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in res:\n",
    "    filename =link.split('=')[1]\n",
    "    url = 'http://www.setn.com/News.aspx?NewsID='+ filename\n",
    "    print(url)\n",
    "    \n",
    "    # using the fucntions defined above ^^^^^^^^^^^^^\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/政治/detail_page/' + filename, text)\n",
    "    #透過分詞重新整理網址內容為最簡短化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]  \n",
    "a = paper[0]\n",
    "res_dict = {}\n",
    "for paper_data in glob.glob('/home/ubuntu/SETN_data/政治/detail_page/*'):\n",
    "    with open(paper_data) as t:\n",
    "        html = t.read()\n",
    "    paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]\n",
    "    detail_paper = \"  \"\n",
    "    if a in paper :\n",
    "        paper.remove(a)\n",
    "    for i in paper :\n",
    "        each_paper = str(i).split('>')[1].split('<')[0]\n",
    "        detail_paper = detail_paper + each_paper\n",
    "        name = paper_data.split('page/')[1]\n",
    "        res_dict[name] = detail_paper\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 社會"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=41&p=%s\"\n",
    "\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/社會/list_page\n",
    "#新建資料夾為儲存所用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=41&p=%s\"\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(\"[INFO]crawling %s\"%url)\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/社會/list_page/' + \"%s.html\"%page, text)\n",
    "    \n",
    "#透過上序方法將跑出的所有分頁儲存在資料夾中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "\n",
    "glob.glob('/home/ubuntu/SETN_data/社會/list_page/*')\n",
    "\n",
    "res = []\n",
    "for filename in glob.glob('/home/ubuntu/SETN_data/社會/list_page/*'):\n",
    "    with open(filename) as f:\n",
    "        html = f.read()\n",
    "        detail_urls = re.findall('''(/News.aspx\\?NewsID=.+)\" style.+''', html)\n",
    "        res.extend(detail_urls)\n",
    "        #/News.aspx?NewsID=377737\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/社會/detail_page\n",
    "#新增一資料夾為儲存內工作內容所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in res:\n",
    "    filename =link.split('=')[1]\n",
    "    url = 'http://www.setn.com/News.aspx?NewsID='+ filename\n",
    "    print(url)\n",
    "    \n",
    "    # using the fucntions defined above ^^^^^^^^^^^^^\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/社會/detail_page/' + filename, text)\n",
    "    #透過分詞重新整理網址內容為最簡短化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]  \n",
    "a = paper[0]\n",
    "res_dict = {}\n",
    "for paper_data in glob.glob('/home/ubuntu/SETN_data/政治/detail_page/*'):\n",
    "    with open(paper_data) as t:\n",
    "        html = t.read()\n",
    "    paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]\n",
    "    if a in paper :\n",
    "        paper.remove(a)\n",
    "    detail_paper = \"  \"\n",
    "    for i in paper :\n",
    "        each_paper = str(i).split('>')[1].split('<')[0]\n",
    "        detail_paper = detail_paper + each_paper\n",
    "        name = paper_data.split('page/')[1]\n",
    "        res_dict[name] = detail_paper\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生活"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=4&p=%s\"\n",
    "\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/生活/list_page\n",
    "#新建資料夾為儲存所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=4&p=%s\"\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(\"[INFO]crawling %s\"%url)\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/生活/list_page/' + \"%s.html\"%page, text)\n",
    "    \n",
    "#透過上序方法將跑出的所有分頁儲存在資料夾中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "\n",
    "glob.glob('/home/ubuntu/SETN_data/生活/list_page/*')\n",
    "\n",
    "res = []\n",
    "for filename in glob.glob('/home/ubuntu/SETN_data/生活/list_page/*'):\n",
    "    with open(filename) as f:\n",
    "        html = f.read()\n",
    "        detail_urls = re.findall('''(/News.aspx\\?NewsID=.+)\" style.+''', html)\n",
    "        res.extend(detail_urls)\n",
    "        #/News.aspx?NewsID=377953\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/生活/detail_page\n",
    "#新增一資料夾為儲存內工作內容所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in res:\n",
    "    filename =link.split('=')[1]\n",
    "    url = 'http://www.setn.com/News.aspx?NewsID='+ filename\n",
    "    print(url)\n",
    "    \n",
    "    # using the fucntions defined above ^^^^^^^^^^^^^\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/生活/detail_page/' + filename, text)\n",
    "    #透過分詞重新整理網址內容為最簡短化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]  \n",
    "a = paper[0]\n",
    "res_dict = {}\n",
    "for paper_data in glob.glob('/home/ubuntu/SETN_data/政治/detail_page/*'):\n",
    "    with open(paper_data) as t:\n",
    "        html = t.read()\n",
    "    paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]\n",
    "    if a in paper :\n",
    "        paper.remove(a)\n",
    "    detail_paper = \"  \"\n",
    "    for i in paper :\n",
    "        each_paper = str(i).split('>')[1].split('<')[0]\n",
    "        detail_paper = detail_paper + each_paper\n",
    "        name = paper_data.split('page/')[1]\n",
    "        res_dict[name] = detail_paper\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 旅遊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=50&p=%s\"\n",
    "\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/旅遊/list_page\n",
    "#新建資料夾為儲存所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=50&p=%s\"\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(\"[INFO]crawling %s\"%url)\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/旅遊/list_page/' + \"%s.html\"%page, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "\n",
    "glob.glob('/home/ubuntu/SETN_data/旅遊/list_page/*')\n",
    "\n",
    "res = []\n",
    "for filename in glob.glob('/home/ubuntu/SETN_data/旅遊/list_page/*'):\n",
    "    with open(filename) as f:\n",
    "        html = f.read()\n",
    "        detail_urls = re.findall('''/travel.setn.com(/News/.+)\" style=.+''', html)\n",
    "        res.extend(detail_urls)\n",
    "        #//travel.setn.com/News/377694\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/旅遊/detail_page\n",
    "#新增一資料夾為儲存內工作內容所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in res:\n",
    "    filename =link.split('News/')[1].split('\\'')[0]\n",
    "    url = 'http://travel.setn.com/News/'+ filename\n",
    "    print(url)\n",
    "    \n",
    "    # using the fucntions defined above ^^^^^^^^^^^^^\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/旅遊/detail_page/' + filename, text)\n",
    "    #透過分詞重新整理網址內容為最簡短化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]  \n",
    "#a = paper[0]\n",
    "res_dict = {}\n",
    "for paper_data in glob.glob('/home/ubuntu/SETN_data/旅遊/detail_page/*'):\n",
    "    with open(paper_data) as t:\n",
    "        html = t.read()\n",
    "    paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]\n",
    "    #if a in paper :\n",
    "        #paper.remove(a)\n",
    "    detail_paper = \"  \"\n",
    "    for i in paper :\n",
    "        each_paper = str(i).split('>')[1].split('<')[0]\n",
    "        detail_paper = detail_paper + each_paper\n",
    "        name = paper_data.split('page/')[1]\n",
    "        res_dict[name] = detail_paper\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 寵物"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=47&p=%s\"\n",
    "\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/寵物/list_page\n",
    "#新建資料夾為儲存所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=47&p=%s\"\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(\"[INFO]crawling %s\"%url)\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/寵物/list_page/' + \"%s.html\"%page, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "\n",
    "glob.glob('/home/ubuntu/SETN_data/寵物/list_page/*')\n",
    "\n",
    "res = []\n",
    "for filename in glob.glob('/home/ubuntu/SETN_data/寵物/list_page/*'):\n",
    "    with open(filename) as f:\n",
    "        html = f.read()\n",
    "        detail_urls = re.findall('''(/News.aspx\\?NewsID=.+)\" style.+''', html)\n",
    "        res.extend(detail_urls)\n",
    "        #/News.aspx?NewsID=377633\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/寵物/detail_page\n",
    "#新增一資料夾為儲存內工作內容所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in res:\n",
    "    filename =link.split('=')[1]\n",
    "    url = 'http://www.setn.com/News.aspx?NewsID='+ filename\n",
    "    print(url)\n",
    "    \n",
    "    # using the fucntions defined above ^^^^^^^^^^^^^\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/寵物/detail_page/' + filename, text)\n",
    "    #透過分詞重新整理網址內容為最簡短化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]  \n",
    "a = paper[0]\n",
    "res_dict = {}\n",
    "for paper_data in glob.glob('/home/ubuntu/SETN_data/寵物/detail_page/*'):\n",
    "    with open(paper_data) as t:\n",
    "        html = t.read()\n",
    "    paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]\n",
    "    if a in paper :\n",
    "        paper.remove(a)\n",
    "    detail_paper = \"  \"\n",
    "    for i in paper :\n",
    "        each_paper = str(i).split('>')[1].split('<')[0]\n",
    "        detail_paper = detail_paper + each_paper\n",
    "        name = paper_data.split('page/')[1]\n",
    "        res_dict[name] = detail_paper\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 國際"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=5&p=%s\"\n",
    "\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/國際/list_page\n",
    "#新建資料夾為儲存所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=5&p=%s\"\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(\"[INFO]crawling %s\"%url)\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/國際/list_page/' + \"%s.html\"%page, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "\n",
    "glob.glob('/home/ubuntu/SETN_data/國際/list_page/*')\n",
    "\n",
    "res = []\n",
    "for filename in glob.glob('/home/ubuntu/SETN_data/國際/list_page/*'):\n",
    "    with open(filename) as f:\n",
    "        html = f.read()\n",
    "        detail_urls = re.findall('''(/News.aspx\\?NewsID=.+)\" style.+''', html)\n",
    "        res.extend(detail_urls)\n",
    "        #/News.aspx?NewsID=377938\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/國際/detail_page\n",
    "#新增一資料夾為儲存內工作內容所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in res:\n",
    "    filename =link.split('=')[1]\n",
    "    url = 'http://www.setn.com/News.aspx?NewsID='+ filename\n",
    "    print(url)\n",
    "    \n",
    "    # using the fucntions defined above ^^^^^^^^^^^^^\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/國際/detail_page/' + filename, text)\n",
    "    #透過分詞重新整理網址內容為最簡短化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]  \n",
    "a = paper[0]\n",
    "res_dict = {}\n",
    "for paper_data in glob.glob('/home/ubuntu/SETN_data/國際/detail_page/*'):\n",
    "    with open(paper_data) as t:\n",
    "        html = t.read()\n",
    "    paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]\n",
    "    if a in paper :\n",
    "        paper.remove(a)\n",
    "    detail_paper = \"  \"\n",
    "    for i in paper :\n",
    "        each_paper = str(i).split('>')[1].split('<')[0]\n",
    "        detail_paper = detail_paper + each_paper\n",
    "        name = paper_data.split('page/')[1]\n",
    "        res_dict[name] = detail_paper\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 科技"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=7&p=%s\"\n",
    "\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/科技/list_page\n",
    "#新建資料夾為儲存所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=7&p=%s\"\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(\"[INFO]crawling %s\"%url)\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/科技/list_page/' + \"%s.html\"%page, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "\n",
    "glob.glob('/home/ubuntu/SETN_data/科技/list_page/*')\n",
    "\n",
    "res = []\n",
    "for filename in glob.glob('/home/ubuntu/SETN_data/科技/list_page/*'):\n",
    "    with open(filename) as f:\n",
    "        html = f.read()\n",
    "        detail_urls = re.findall('''(/News.aspx\\?NewsID=.+)\" style.+''', html)\n",
    "        res.extend(detail_urls)\n",
    "        #/News.aspx?NewsID=377844\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/科技/detail_page\n",
    "#新增一資料夾為儲存內工作內容所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in res:\n",
    "    filename =link.split('=')[1]\n",
    "    url = 'http://www.setn.com/News.aspx?NewsID='+ filename\n",
    "    print(url)\n",
    "    \n",
    "    # using the fucntions defined above ^^^^^^^^^^^^^\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/科技/detail_page/' + filename, text)\n",
    "    #透過分詞重新整理網址內容為最簡短化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]  \n",
    "a = paper[0]\n",
    "res_dict = {}\n",
    "for paper_data in glob.glob('/home/ubuntu/SETN_data/科技/detail_page/*'):\n",
    "    with open(paper_data) as t:\n",
    "        html = t.read()\n",
    "    paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]\n",
    "    if a in paper :\n",
    "        paper.remove(a)\n",
    "    detail_paper = \"  \"\n",
    "    for i in paper :\n",
    "        each_paper = str(i).split('>')[1].split('<')[0]\n",
    "        detail_paper = detail_paper + each_paper\n",
    "        name = paper_data.split('page/')[1]\n",
    "        res_dict[name] = detail_paper\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新奇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=42&p=%s\"\n",
    "\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/新奇/list_page\n",
    "#新建資料夾為儲存所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=42&p=%s\"\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(\"[INFO]crawling %s\"%url)\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/新奇/list_page/' + \"%s.html\"%page, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "\n",
    "glob.glob('/home/ubuntu/SETN_data/新奇/list_page/*')\n",
    "\n",
    "res = []\n",
    "for filename in glob.glob('/home/ubuntu/SETN_data/新奇/list_page/*'):\n",
    "    with open(filename) as f:\n",
    "        html = f.read()\n",
    "        detail_urls = re.findall('''(/News.aspx\\?NewsID=.+)\" style.+''', html)\n",
    "        res.extend(detail_urls)\n",
    "        #/News.aspx?NewsID=377545\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/新奇/detail_page\n",
    "#新增一資料夾為儲存內工作內容所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in res:\n",
    "    filename =link.split('=')[1]\n",
    "    url = 'http://www.setn.com/News.aspx?NewsID='+ filename\n",
    "    print(url)\n",
    "    \n",
    "    # using the fucntions defined above ^^^^^^^^^^^^^\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/新奇/detail_page/' + filename, text)\n",
    "    #透過分詞重新整理網址內容為最簡短化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]  \n",
    "a = paper[0]\n",
    "res_dict = {}\n",
    "for paper_data in glob.glob('/home/ubuntu/SETN_data/新奇/detail_page/*'):\n",
    "    with open(paper_data) as t:\n",
    "        html = t.read()\n",
    "    paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]\n",
    "    if a in paper :\n",
    "        paper.remove(a)\n",
    "    detail_paper = \"  \"\n",
    "    for i in paper :\n",
    "        each_paper = str(i).split('>')[1].split('<')[0]\n",
    "        detail_paper = detail_paper + each_paper\n",
    "        name = paper_data.split('page/')[1]\n",
    "        res_dict[name] = detail_paper\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 運動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=34&p=%s\"\n",
    "\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/運動/list_page\n",
    "#新建資料夾為儲存所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=34&p=%s\"\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(\"[INFO]crawling %s\"%url)\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/運動/list_page/' + \"%s.html\"%page, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "\n",
    "glob.glob('/home/ubuntu/SETN_data/運動/list_page/*')\n",
    "\n",
    "res = []\n",
    "for filename in glob.glob('/home/ubuntu/SETN_data/運動/list_page/*'):\n",
    "    with open(filename) as f:\n",
    "        html = f.read()\n",
    "        detail_urls = re.findall('''(/News.aspx\\?NewsID=.+)\" style.+''', html)\n",
    "        res.extend(detail_urls)\n",
    "        #/News.aspx?NewsID=377888\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/運動/detail_page\n",
    "#新增一資料夾為儲存內工作內容所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in res:\n",
    "    filename =link.split('=')[1]\n",
    "    url = 'http://www.setn.com/News.aspx?NewsID='+ filename\n",
    "    print(url)\n",
    "    \n",
    "    # using the fucntions defined above ^^^^^^^^^^^^^\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/運動/detail_page/' + filename, text)\n",
    "    #透過分詞重新整理網址內容為最簡短化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]  \n",
    "a = paper[0]\n",
    "res_dict = {}\n",
    "for paper_data in glob.glob('/home/ubuntu/SETN_data/運動/detail_page/*'):\n",
    "    with open(paper_data) as t:\n",
    "        html = t.read()\n",
    "    paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]\n",
    "    if a in paper :\n",
    "        paper.remove(a)\n",
    "    detail_paper = \"  \"\n",
    "    for i in paper :\n",
    "        each_paper = str(i).split('>')[1].split('<')[0]\n",
    "        detail_paper = detail_paper + each_paper\n",
    "        name = paper_data.split('page/')[1]\n",
    "        res_dict[name] = detail_paper\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 財經"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=2&p=%s\"\n",
    "\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/財經/list_page\n",
    "#新建資料夾為儲存所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=2&p=%s\"\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(\"[INFO]crawling %s\"%url)\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/財經/list_page/' + \"%s.html\"%page, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "\n",
    "glob.glob('/home/ubuntu/SETN_data/財經/list_page/*')\n",
    "\n",
    "res = []\n",
    "for filename in glob.glob('/home/ubuntu/SETN_data/財經/list_page/*'):\n",
    "    with open(filename) as f:\n",
    "        html = f.read()\n",
    "        detail_urls = re.findall('''(/News.aspx\\?NewsID=.+)\" style.+''', html)\n",
    "        res.extend(detail_urls)\n",
    "        #/News.aspx?NewsID=378008\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/財經/detail_page\n",
    "#新增一資料夾為儲存內工作內容所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in res:\n",
    "    filename =link.split('=')[1]\n",
    "    url = 'http://www.setn.com/News.aspx?NewsID='+ filename\n",
    "    print(url)\n",
    "    \n",
    "    # using the fucntions defined above ^^^^^^^^^^^^^\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/財經/detail_page/' + filename, text)\n",
    "    #透過分詞重新整理網址內容為最簡短化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]  \n",
    "a = paper[0]\n",
    "res_dict = {}\n",
    "for paper_data in glob.glob('/home/ubuntu/SETN_data/財經/detail_page/*'):\n",
    "    with open(paper_data) as t:\n",
    "        html = t.read()\n",
    "    paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]\n",
    "    if a in paper :\n",
    "        paper.remove(a)\n",
    "    detail_paper = \"  \"\n",
    "    for i in paper :\n",
    "        each_paper = str(i).split('>')[1].split('<')[0]\n",
    "        detail_paper = detail_paper + each_paper\n",
    "        name = paper_data.split('page/')[1]\n",
    "        res_dict[name] = detail_paper\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 華語流行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=17&p=%s\"\n",
    "\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/華語流行/list_page\n",
    "#新建資料夾為儲存所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=17&p=%s\"\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(\"[INFO]crawling %s\"%url)\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/華語流行/list_page/' + \"%s.html\"%page, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "\n",
    "glob.glob('/home/ubuntu/SETN_data/華語流行/list_page/*')\n",
    "\n",
    "res = []\n",
    "for filename in glob.glob('/home/ubuntu/SETN_data/華語流行/list_page/*'):\n",
    "    with open(filename) as f:\n",
    "        html = f.read()\n",
    "        detail_urls = re.findall('''(/E/News.aspx\\?NewsID=.+)\" style.+''', html)\n",
    "        res.extend(detail_urls)\n",
    "        #/E/News.aspx?NewsID=375297\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/華語流行/detail_page\n",
    "#新增一資料夾為儲存內工作內容所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in res:\n",
    "    filename =link.split('=')[1]\n",
    "    url = 'http://www.setn.com/News.aspx?NewsID='+ filename\n",
    "    print(url)\n",
    "    \n",
    "    # using the fucntions defined above ^^^^^^^^^^^^^\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/華語流行/detail_page/' + filename, text)\n",
    "    #透過分詞重新整理網址內容為最簡短化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]  \n",
    "a = paper[0]\n",
    "res_dict = {}\n",
    "for paper_data in glob.glob('/home/ubuntu/SETN_data/華語流行/detail_page/*'):\n",
    "    with open(paper_data) as t:\n",
    "        html = t.read()\n",
    "    paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]\n",
    "    if a in paper :\n",
    "        paper.remove(a)\n",
    "    detail_paper = \"  \"\n",
    "    for i in paper :\n",
    "        each_paper = str(i).split('>')[1].split('<')[0]\n",
    "        detail_paper = detail_paper + each_paper\n",
    "        name = paper_data.split('page/')[1]\n",
    "        res_dict[name] = detail_paper\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 音樂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=46&p=%s\"\n",
    "\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/音樂/list_page\n",
    "#新建資料夾為儲存所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.setn.com/ViewAll.aspx?PageGroupID=46&p=%s\"\n",
    "for page in range(1,21):\n",
    "    url = url_template%page\n",
    "    print(\"[INFO]crawling %s\"%url)\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/音樂/list_page/' + \"%s.html\"%page, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "\n",
    "glob.glob('/home/ubuntu/SETN_data/音樂/list_page/*')\n",
    "\n",
    "res = []\n",
    "for filename in glob.glob('/home/ubuntu/SETN_data/音樂/list_page/*'):\n",
    "    with open(filename) as f:\n",
    "        html = f.read()\n",
    "        detail_urls = re.findall('''(/E/News.aspx\\?NewsID=.+)\" style.+''', html)\n",
    "        res.extend(detail_urls)\n",
    "        #/E/News.aspx?NewsID=377609\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ubuntu/SETN_data/音樂/detail_page\n",
    "#新增一資料夾為儲存內工作內容所用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in res:\n",
    "    filename =link.split('=')[1]\n",
    "    url = 'http://www.setn.com/News.aspx?NewsID='+ filename\n",
    "    print(url)\n",
    "    \n",
    "    # using the fucntions defined above ^^^^^^^^^^^^^\n",
    "    text = get_page(url)\n",
    "    save2file('/home/ubuntu/SETN_data/音樂/detail_page/' + filename, text)\n",
    "    #透過分詞重新整理網址內容為最簡短化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]  \n",
    "a = paper[0]\n",
    "res_dict = {}\n",
    "for paper_data in glob.glob('/home/ubuntu/SETN_data/音樂/detail_page/*'):\n",
    "    with open(paper_data) as t:\n",
    "        html = t.read()\n",
    "    paper = [ele for ele in BeautifulSoup(html, 'lxml').select('div > p')]\n",
    "    if a in paper :\n",
    "        paper.remove(a)\n",
    "    detail_paper = \"  \"\n",
    "    for i in paper :\n",
    "        each_paper = str(i).split('>')[1].split('<')[0]\n",
    "        detail_paper = detail_paper + each_paper\n",
    "        name = paper_data.split('page/')[1]\n",
    "        res_dict[name] = detail_paper\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
